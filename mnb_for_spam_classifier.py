# -*- coding: utf-8 -*-
"""MNB for spam classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LWKMXYpI2nSRL-V3wF0XFgqIjIRh96wS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import operator
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

from google.colab import drive
drive.mount('/content/drive/')

train_label = pd.read_csv('/content/drive/My Drive/SMSSpamCollection', sep='\t',
                           names=["label", "message"])



print(train_label)
print(len(train_label))

corpus=[]
for line in range(0, len(train_label)):
  review = re.sub('[^a-zA-Z]', ' ', train_label['message'][line])
  review = review.lower()
  review = review.split()
    
  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
  review = ' '.join(review) 
  corpus.append(review)   

print(corpus[1])
print(len(corpus[1]))

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2500)
X = cv.fit_transform(corpus).toarray()
print(cv.get_feature_names())
y=pd.get_dummies(train_label['label'])
y=y.iloc[:,1].values
print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)
print(X_train.shape)
print(X_test.shape)

p={}
p[0]=0
p[1]=0
for prior in range(len(y_train)):  
  if y_train[prior]==0:
    p[0]=p[0]+1
p[0]=p[0]/len(y_train)
p[1]=1-p[0]

print(p[0])
print(p[1])


dr0=0

for m in range(len(X_train)):
  if y_train[m]==0:
    for w in range(len(X_train[m])):
      dr0=dr0+X_train[m][w]
    
print(dr0)   


dr1=0
for m in range(len(X_train)):
  if y_train[m]==1:
    for w in range(len(X_train[m])):
      dr1=dr1+X_train[m][w]
    
print(dr1)

ans=[]            #belonging to class 0
for k in range(len(y_test)):
  ans.append(1)
for sentence in range(len(X_test)):
  for word in range(len(X_test[sentence])):
    count=0
    if X_test[sentence][word]!=0:
      for sent in range(len(X_train)):
        if X_train[sent][word]!=0 and y_train[sent]==0:
          count=count+X_train[sent][word]
          
      ans[sentence]=ans[sentence]*((count+0.001)/(dr0+0.001*2500))** X_test[sentence][word]      



var=[]
for k in range(len(y_test)):
  var.append(1)
for sentence in range(len(X_test)):
  for word in range(len(X_test[sentence])):
    count=0
    if X_test[sentence][word]!=0:
      for sent in range(len(X_train)):
        if X_train[sent][word]!=0 and y_train[sent]==1:
          count=count+X_train[sent][word]
      var[sentence]=var[sentence]*((count+0.001)/(dr1+0.001*2500))**X_test[sentence][word]     

for k in range(len(y_test)):
  ans[k]=ans[k]/(ans[k]+var[k])
  var[k]=var[k]/(ans[k]+var[k])

print(ans)
print(var)


loss=0

y_predicted=[]
for k in range(len(y_test)):
  if(ans[k]*p[0]>var[k]*p[1]):
    y_predicted.append(0)
    loss=loss+var[k]*p[1]/(var[k]*p[1]+ans[k]*p[0])
  else:
    y_predicted.append(1)  
    loss=loss+ans[k]*p[0]/(var[k]*p[1]+ans[k]*p[0])

from sklearn.metrics import confusion_matrix
conf_mat = confusion_matrix(y_predicted, y_test)
acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)
print('Overall accuracy: {} %'.format(acc*100))
print('Overall loss: {} '.format(loss))

