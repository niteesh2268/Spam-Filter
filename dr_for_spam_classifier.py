# -*- coding: utf-8 -*-
"""DR

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qwEZo6L9aFFYrsY0b6mfXru8nBYVn3ge
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import operator
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

from google.colab import drive
drive.mount('/content/drive/')

train_label = pd.read_csv('/content/drive/My Drive/SMSSpamCollection', sep='\t',
                           names=["label", "message"])

print(train_label)
print(len(train_label))

corpus=[]
for line in range(0, len(train_label)):
  review = re.sub('[^a-zA-Z]', ' ', train_label['message'][line])
  review = review.lower()
  review = review.split()
    
  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
  review = ' '.join(review)
  corpus.append(review)

print(corpus[1])
print(len(corpus[1]))

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=1000)
X = cv.fit_transform(corpus).toarray()
X=X+0.01
print(X)
# print(cv.get_feature_names())
y=pd.get_dummies(train_label['label'])
y=y.iloc[:,1].values
print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)
print(X_train.shape)
print(X_test.shape)

p={}
p[0]=0
p[1]=0
for prior in range(len(y_train)):  
  if y_train[prior]==0:
    p[0]=p[0]+1
p[0]=p[0]/len(y_train)
p[1]=1-p[0]

print(p[0])
print(p[1])

!pip install git+https://github.com/ericsuh/dirichlet.git

import numpy
import dirichlet
t=[]


print(y_train)

for sentence in range(len(X_train)):
  if y_train[sentence]==0:
    sum=0
    for word in range(len(X_train[sentence])):
      sum=sum+X_train[sentence][word]
    t.append(X_train[sentence]/sum)

# print(len(t))

b=numpy.asarray(t)
# print(b.shape)
a0=dirichlet.mle(b)
print(a0)

v=[]


# print(X_train)

for sentence in range(len(X_train)):
  if y_train[sentence]==1:
    sum=0
    for word in range(len(X_train[sentence])):
      sum=sum+X_train[sentence][word]
    v.append(X_train[sentence]/sum)
# print(len(t))

b=numpy.asarray(v)
# print(b.shape)
a1=dirichlet.mle(b)
print(a1)

ans=[]
for k in range(len(X_test)):
  ans.append(1)

for sentence in range(len(X_test)):
  ls=[]
  for word in range(len(X_test[sentence])):
    ls.append(X_test[sentence][word])
  tsum=0 
  for k in range(len(ls)):
    tsum=tsum+ls[k] 
  for k in range(len(X_test[sentence])):
    temp=(a0[k]-1)*np.log(ls[k]/tsum)
    ans[sentence]=ans[sentence]+temp


print(ans)

var=[]
for k in range(len(X_test)):
  var.append(1)

for sentence in range(len(X_test)):
  ls=[]
  for word in range(len(X_test[sentence])):
    ls.append(X_test[sentence][word])
  tsum=0 
  for k in range(len(ls)):
    tsum=tsum+ls[k] 
  for k in range(len(X_test[sentence])):
    temp=(a1[k]-1)*np.log(ls[k]/tsum)
    var[sentence]=var[sentence]+temp


print(var)

from scipy.special import gammaln,softmax,gamma

l=gammaln(a0.sum())-gammaln(a0).sum()
for k in range(len(X_test)):
  ans[k]=ans[k]+l
print(ans)

l=gammaln(a1.sum())-gammaln(a1).sum()
for k in range(len(X_test)):
  var[k]=var[k]+l
print(var)

loss=0

y_predicted=[]
for k in range(len(y_test)):
  if((ans[k]+np.log(p[0]))>(var[k]+np.log(p[1]))):
    y_predicted.append(0)
    loss=loss+var[k]+np.log(p[1])/(var[k]+np.log(p[1])+ans[k]+np.log(p[0]))
  else:
    y_predicted.append(1)  
    loss=loss+ans[k]+np.log(p[0])/(var[k]+np.log(p[1])+ans[k]+np.log(p[0]))

from sklearn.metrics import confusion_matrix
conf_mat = confusion_matrix(y_predicted, y_test)
acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)
print('Overall accuracy: {} %'.format(acc*100))
print('Overall loss: {} '.format(loss))

