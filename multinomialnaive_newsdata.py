# -*- coding: utf-8 -*-
"""MultinomialNaive_NewsData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k47-PEh5Gab_UXxvQMWkcysqKCUUxRMV
"""

import numpy as np
import pandas as pd
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import operator
nltk.download('stopwords')
from google.colab import drive
drive.mount('/content/drive/')

#calculating prior probabilites
path_to_dataset = '/content/drive/My Drive/statisticalPR/datasets/NewsDataSet/'
train_label = open(path_to_dataset+'train.label')
iter_label = train_label.readlines()
prior_probs = {}
for i in range(1,21):
    prior_probs[i]=0
for i in iter_label:
    prior_probs[int(i)]=prior_probs[int(i)]+1
for key in prior_probs:
    prior_probs[key]=prior_probs[key]/len(iter_label)
    
print("Probability of each class:")
print("\n".join("{}: {}".format(k, v) for k, v in prior_probs.items()))

#finding the relevant vocabulary
stop_words = set(stopwords.words('english'))
vocab = open(path_to_dataset+'vocabulary.txt') 
iter_vocab = vocab.readlines()
dict_vocab = {}
for idx, i in enumerate(iter_vocab):
    dict_vocab[idx+1]=i

dict_good_vocab = {}
for key in dict_vocab:
    if dict_vocab[key][:-1] in stop_words: dict_good_vocab[key]=0
    else : dict_good_vocab[key]=1

#finding the most relevant vocabulary
train_data = open(path_to_dataset+'train.data')
iter_train_data = train_data.readlines()
vocab_freq={}
for key in dict_vocab:
    vocab_freq[key]=0

for i in iter_train_data:
    data = i.split()
    vocab_freq[int(data[1])]=vocab_freq[int(data[1])]+int(data[2])
sorted_vocab_freq = sorted(vocab_freq.items(), key=operator.itemgetter(1),reverse=True)
relevant_vocab=[]
n=500
for i in range(n):
    key = sorted_vocab_freq[i][0]
    if dict_good_vocab[key]==1:
        relevant_vocab.append(key)

#creating a pandas dataframe to visualize data

train_data = open(path_to_dataset+'train.data')
df = pd.read_csv(train_data,delimiter=' ',names=['docIdx','wordIdx','count'])
label=[]
for i in iter_label:
    label.append(int(i.split()[0]))
docIdx = df['docIdx'].values
i=0
new_label = []
for index in range(len(docIdx)-1):
    new_label.append(label[i])
    if docIdx[index] != docIdx[index+1]:
        i += 1
new_label.append(label[i]) #for-loop ignores last value
#Add label column
df['classIdx'] = new_label
print(df.head())

df=df[df['wordIdx'].isin(relevant_vocab)]
print(df.head())
print(len(relevant_vocab))

#Calculate probability of each word based on class
pb_ij = df.groupby(['classIdx','wordIdx'])
pb_j = df.groupby(['classIdx'])
#laplace smoothing
a=0.001
Pr = (pb_ij['count'].sum() + a) / (pb_j['count'].sum()+a*len(relevant_vocab))    

#Unstack series
Pr = Pr.unstack()

#Replace NaN or columns with 0 as word count with a/(count+|V|+1)
for c in range(1,21):
    Pr.loc[c,:] = Pr.loc[c,:].fillna(a/(pb_j['count'].sum()[c]))

#Convert to dictionary for greater speed
Pr_dict = Pr.to_dict()
Pr

def MNB(df):
    df_dict = df.to_dict()
    new_dict = {}
    prediction = []
    #new_dict = {docIdx : {wordIdx: count},....}
    for idx in range(len(df_dict['docIdx'])):
        docIdx = df_dict['docIdx'][idx]
        wordIdx = df_dict['wordIdx'][idx]
        count = df_dict['count'][idx]
        try:
          new_dict[docIdx][wordIdx] = count
        except:
          #for unseen words in training i,e all the removed words
          new_dict[df_dict['docIdx'][idx]] = {}
          new_dict[docIdx][wordIdx] = count 
    #Calculating the scores for each doc
    for docIdx in range(1, len(new_dict)+1):
        score_dict = {}
        #Creating a probability row for each class
        for classIdx in range(1,21):
            score_dict[classIdx] = 1
            #For each word:
            for wordIdx in new_dict[docIdx]:
              try:
                probability = Pr_dict[wordIdx][classIdx]        
                power = new_dict[docIdx][wordIdx]               
                score_dict[classIdx]+=power*np.log(probability)    
              except:
                #unseen words will have score 0
                score_dict[classIdx] += 0   
            #adding prior probs         
            score_dict[classIdx] +=  np.log(prior_probs[classIdx])                          

        #Get class with max probabilty for the given docIdx 
        max_score = max(score_dict, key=score_dict.get)
        prediction.append(max_score)
        
    return prediction

#Get test data
test_data = open(path_to_dataset+'test.data')
df = pd.read_csv(test_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])

#Get list of labels
test_label = pd.read_csv(path_to_dataset+'test.label', names=['t'])
test_label= test_label['t'].tolist()

#MNB Calculation
predict = MNB(df)
total = len(test_label)
val = 0
for i,j in zip(predict, test_label):
    if i == j:
        val +=1
    else:
        pass
print("Error:\t",(1-(val/total)) * 100, "%")

#no_of_features={1072:36.9620;2067:30.766155;579:44.2638}
accuracy = {385:51.5922,872:39.1205,1868:31.8720,2866:29.1405,3865:27.5284,4863:26.369,5862:25.9826,6861:25.2498,9860:24.0506,19859:21.6389,29856:21.2924,39856:20.83944,49856:20.5063}
#118,128,132,134,135,137,138,140,140,141,144
print(accuracy)
#plt.hist(accuracy.keys(), accuracy.values(), color='g', label = "Real distribution")
#plt.show()